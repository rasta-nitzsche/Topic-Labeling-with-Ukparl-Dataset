{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the data from txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files from the dataset are empty so to avoid any problem we catch it and ignore the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Files skipped\n",
      "2 Files skipped\n",
      "3 Files skipped\n",
      "4 Files skipped\n",
      "5 Files skipped\n",
      "6 Files skipped\n",
      "7 Files skipped\n",
      "8 Files skipped\n",
      "9 Files skipped\n",
      "10 Files skipped\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.chdir('C:/Users/Rasta/Desktop/Topic Labelling/2014_2013')\n",
    "a= []\n",
    "i=1\n",
    "\n",
    "for f in os.listdir():\n",
    "    os.chdir('C:/Users/Rasta/Desktop/Topic Labelling/2014_2013/' + str(f)+ '/text')\n",
    "    for d in os.listdir():\n",
    "        try:\n",
    "            file1 = open(d, 'r',encoding=\"mbcs\")\n",
    "        except FileNotFoundError:\n",
    "            print (str(i) + \" Files skipped\")\n",
    "            i=i+1\n",
    "        Lines = file1.readlines()\n",
    "        for line in Lines:\n",
    "            a.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That the sitting be now adjourned.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We are often at our best when we are sharing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I met one of my local ambulance service chief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I find it incredible that the shadow Minister...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163155</th>\n",
       "      <td>Despite having published a paper specifically...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163156</th>\n",
       "      <td>I am pretty certain that any answers that wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163157</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163158</th>\n",
       "      <td>Surely one of the great weaknesses of the Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163159</th>\n",
       "      <td>The Prime Minister recently visited Vent-Axia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163160 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0                    That the sitting be now adjourned.\\n\n",
       "1        We are often at our best when we are sharing ...\n",
       "2                                                      \\n\n",
       "3        I met one of my local ambulance service chief...\n",
       "4        I find it incredible that the shadow Minister...\n",
       "...                                                   ...\n",
       "163155   Despite having published a paper specifically...\n",
       "163156   I am pretty certain that any answers that wou...\n",
       "163157                                                 \\n\n",
       "163158   Surely one of the great weaknesses of the Whi...\n",
       "163159  The Prime Minister recently visited Vent-Axia ...\n",
       "\n",
       "[163160 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing \"\\n\" and reseting the index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[0]!='\\n'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data[0].str.replace(\"\\n\",\"\").reset_index()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing NTLK library for natural language processing and downloading additional needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rasta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rasta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a filter function to tokeniz and  remove stepwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(ex):\n",
    "    word_tokens = word_tokenize(ex)\n",
    "    filtered = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join([str(elem) for elem in filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function (Seems to work fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sample sentence , testing validity function , let 's go Pivony !\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(\"\"\"This is a sample sentence,\n",
    "                  I am testing the validity of my function , let's go Pivony !\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_data = data[0].apply(lambda x:filter(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sitting adjourned .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>often best sharing personal experiences , pay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>met one local ambulance service chief executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find incredible shadow Minister states issue c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vulnerable patients local hospital beds go for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126845</th>\n",
       "      <td>Indeed , things achieved part United Kingdom ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126846</th>\n",
       "      <td>Despite published paper specifically pensions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126847</th>\n",
       "      <td>pretty certain answers would come nationalists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126848</th>\n",
       "      <td>Surely one great weaknesses White Paper future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126849</th>\n",
       "      <td>Prime Minister recently visited Vent-Axia cons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126850 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0                                     sitting adjourned .\n",
       "1       often best sharing personal experiences , pay ...\n",
       "2       met one local ambulance service chief executiv...\n",
       "3       find incredible shadow Minister states issue c...\n",
       "4       vulnerable patients local hospital beds go for...\n",
       "...                                                   ...\n",
       "126845  Indeed , things achieved part United Kingdom ....\n",
       "126846  Despite published paper specifically pensions ...\n",
       "126847  pretty certain answers would come nationalists...\n",
       "126848  Surely one great weaknesses White Paper future...\n",
       "126849  Prime Minister recently visited Vent-Axia cons...\n",
       "\n",
       "[126850 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokened_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for training as it is an already well prepared dataset (I took only the 2013-2014 files because it take way many time to train on the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this library to create our model. These models implement HuggingFace Transformers and c-TF-IDF to create dense clusters using HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (0.23.2)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (0.8.28)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (2.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (0.5.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (1.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (4.50.2)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (5.3.1)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bertopic) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (0.29.21)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Requirement already satisfied: huggingface-hub in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.0.12)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.9.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.3)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2020.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (2020.10.15)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (3.7.4.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.45)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.0.1)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (1.25.11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(verbose=True, embedding_model=\"all-mpnet-base-v2\",\n",
    "                       min_topic_size=1000,  calculate_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(list(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_model = BERTopic.load(\"tokenized_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will not train the model on this notebook as it will take around 40 hours. I did on Google Collab and saved the model (you will find it attached with my submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
